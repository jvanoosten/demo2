{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "# Trading Platform Customer Attrition Risk Prediction using sklearn\n\nThere are many users of online trading platforms and these companies would like to run analytics on and predict churn based on user activity on the platform. Since competition is rife, keeping customers happy so they do not move their investments elsewhere is key to maintaining profitability.\n\nIn this notebook, we will leverage Watson Studio Local (that is a service on IBM Cloud Pak for Data) to do the following:\n\n1. Ingest merged customer demographics and trading activity data\n2. Visualize merged dataset and get better understanding of data to build hypotheses for prediction\n3. Leverage sklearn library to build classification model that predicts whether customer has propensity to churn\n4. Expose the classification model as RESTful API endpoint for the end-to-end customer churn risk prediction and risk remediation application\n\n<img src=\"https://github.com/burtvialpando/CloudPakWorkshop/blob/master/CPD/images/NotebookImage.png?raw=true\" width=\"800\" height=\"500\" align=\"middle\"/>\n\n\n<a id=\"top\"></a>\n## Table of Contents\n\n1. [Load libraries](#load_libraries)\n2. [Load and visualize merged customer demographics and trading activity data](#load_data)\n3. [Prepare data for building classification model](#prepare_data)\n4. [Train classification model and test model performance](#build_model)\n5. [Save model to ML repository and expose it as REST API endpoint](#save_model)\n6. [Summary](#summary)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Quick set of instructions to work through the notebook\n\nIf you are new to Notebooks, here's a quick overview of how to work in this environment.\n\n1. The notebook has 2 types of cells - markdown (text) such as this and code such as the one below. \n2. Each cell with code can be executed independently or together (see options under the Cell menu). When working in this notebook, we will be running one cell at a time because we need to make code changes to some of the cells.\n3. To run the cell, position cursor in the code cell and click the Run (arrow) icon. The cell is running when you see the * next to it. Some cells have printable output.\n4. Work through this notebook by reading the instructions and executing code cell by cell. Some cells will require modifications before you run them. "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id=\"load_libraries\"></a>\n## 1. Load libraries\n[Top](#top)\n\nRunning the following cell will load all libraries needed to load, visualize, prepare the data and build ML models for our use case"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Collecting sklearn-pandas==1.7.0\n  Downloading sklearn_pandas-1.7.0-py2.py3-none-any.whl (10 kB)\nRequirement already satisfied: scipy>=0.14 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from sklearn-pandas==1.7.0) (1.5.0)\nRequirement already satisfied: numpy>=1.6.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from sklearn-pandas==1.7.0) (1.18.5)\nRequirement already satisfied: scikit-learn>=0.15.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from sklearn-pandas==1.7.0) (0.23.1)\nRequirement already satisfied: pandas>=0.11.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from sklearn-pandas==1.7.0) (1.0.5)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from scikit-learn>=0.15.0->sklearn-pandas==1.7.0) (2.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from scikit-learn>=0.15.0->sklearn-pandas==1.7.0) (0.16.0)\nRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pandas>=0.11.0->sklearn-pandas==1.7.0) (2.8.1)\nRequirement already satisfied: pytz>=2017.2 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pandas>=0.11.0->sklearn-pandas==1.7.0) (2020.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas>=0.11.0->sklearn-pandas==1.7.0) (1.15.0)\nInstalling collected packages: sklearn-pandas\nSuccessfully installed sklearn-pandas-1.7.0\n"
                }
            ],
            "source": "#Uncomment and run once to install the package in your runtime environment\n#!pip uninstall -y sklearn-pandas\n!pip install  --no-cache-dir sklearn-pandas==1.7.0"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Requirement already up-to-date: matplotlib==3.2.2 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (3.2.2)\nRequirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from matplotlib==3.2.2) (2.8.1)\nRequirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from matplotlib==3.2.2) (1.2.0)\nRequirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from matplotlib==3.2.2) (2.4.7)\nRequirement already satisfied, skipping upgrade: numpy>=1.11 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from matplotlib==3.2.2) (1.18.5)\nRequirement already satisfied, skipping upgrade: cycler>=0.10 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from matplotlib==3.2.2) (0.10.0)\nRequirement already satisfied, skipping upgrade: six>=1.5 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib==3.2.2) (1.15.0)\n"
                }
            ],
            "source": "# If the following cell doesn't work, please un-comment out the next line and do upgrade the patplotlib package. When the upgrade is done, restart the kernal and start from the beginning again. \n!pip install --user --upgrade matplotlib==3.2.2"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Collecting brunel==2.3\n  Downloading brunel-2.3.tar.gz (2.1 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.1 MB 16.7 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: pandas in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from brunel==2.3) (1.0.5)\nRequirement already satisfied: jinja2 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from brunel==2.3) (2.11.2)\nRequirement already satisfied: ipython in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from brunel==2.3) (7.15.0)\nRequirement already satisfied: jupyter-pip in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from brunel==2.3) (0.3.1)\nCollecting JPype1-py3\n  Downloading JPype1-py3-0.5.5.4.tar.gz (88 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 88 kB 12.4 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pandas->brunel==2.3) (2020.1)\nRequirement already satisfied: numpy>=1.13.3 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pandas->brunel==2.3) (1.18.5)\nRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pandas->brunel==2.3) (2.8.1)\nRequirement already satisfied: MarkupSafe>=0.23 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from jinja2->brunel==2.3) (1.1.1)\nRequirement already satisfied: pickleshare in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from ipython->brunel==2.3) (0.7.5)\nRequirement already satisfied: pygments in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from ipython->brunel==2.3) (2.6.1)\nRequirement already satisfied: setuptools>=18.5 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from ipython->brunel==2.3) (47.3.1.post20200622)\nRequirement already satisfied: backcall in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from ipython->brunel==2.3) (0.2.0)\nRequirement already satisfied: decorator in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from ipython->brunel==2.3) (4.4.2)\nRequirement already satisfied: pexpect; sys_platform != \"win32\" in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from ipython->brunel==2.3) (4.8.0)\nRequirement already satisfied: traitlets>=4.2 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from ipython->brunel==2.3) (4.3.3)\nRequirement already satisfied: jedi>=0.10 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from ipython->brunel==2.3) (0.17.1)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from ipython->brunel==2.3) (3.0.5)\nRequirement already satisfied: six>=1.5 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas->brunel==2.3) (1.15.0)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->ipython->brunel==2.3) (0.6.0)\nRequirement already satisfied: ipython-genutils in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from traitlets>=4.2->ipython->brunel==2.3) (0.2.0)\nRequirement already satisfied: parso<0.8.0,>=0.7.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from jedi>=0.10->ipython->brunel==2.3) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->brunel==2.3) (0.2.4)\nBuilding wheels for collected packages: brunel, JPype1-py3\n  Building wheel for brunel (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for brunel: filename=brunel-2.3-py3-none-any.whl size=2144778 sha256=16c3a0202545662f32fdb4489cd6539b3c31ecf764d25379d43f4d33e63b290f\n  Stored in directory: /tmp/wsuser/.cache/pip/wheels/2c/9c/42/47a443de8427f784efdb3e46a123b9b955fce789a46db5a8b5\n  Building wheel for JPype1-py3 (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for JPype1-py3: filename=JPype1_py3-0.5.5.4-cp37-cp37m-linux_x86_64.whl size=3660064 sha256=8352f853c2264eadf3f03565a54e1c80d922d89db314cc8ce569a9b7fd3d2fe5\n  Stored in directory: /tmp/wsuser/.cache/pip/wheels/e7/d1/09/f55dca0203b0691945bdf0f63d486a0b4d4e5ec4bd78a2502e\nSuccessfully built brunel JPype1-py3\nInstalling collected packages: JPype1-py3, brunel\nSuccessfully installed JPype1-py3-0.5.5.4 brunel-2.3\n"
                }
            ],
            "source": "!pip install brunel==2.3"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": "import brunel\nimport pandas as pd\nimport numpy as np\nimport sklearn.pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler, LabelBinarizer, OneHotEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, accuracy_score, roc_curve, roc_auc_score\nfrom sklearn_pandas import DataFrameMapper\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\nimport json\nimport matplotlib.pyplot as plt\n%matplotlib inline"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Found existing installation: scikit-learn 0.23.1\nUninstalling scikit-learn-0.23.1:\n  Successfully uninstalled scikit-learn-0.23.1\nCollecting scikit-learn==0.22\n  Downloading scikit_learn-0.22-cp37-cp37m-manylinux1_x86_64.whl (7.0 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7.0 MB 11.1 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from scikit-learn==0.22) (1.5.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from scikit-learn==0.22) (0.16.0)\nRequirement already satisfied: numpy>=1.11.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from scikit-learn==0.22) (1.18.5)\nInstalling collected packages: scikit-learn\nSuccessfully installed scikit-learn-0.22\n"
                }
            ],
            "source": "#Changed sk-learn version to be compatible with WML client4 on CPD v3.0.1\n!pip uninstall -y scikit-learn\n!pip install --no-cache-dir scikit-learn==0.22"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# <a id=\"load_data\"></a>\n## 2. Load data example\n[Top](#top)\n\nData can be easily loaded within ICPD using point-and-click functionality. The following image illustrates how to load a merged dataset assuming it is called \"customer_demochurn_activity_analyze.csv\". The file can be located by its name and inserted into the notebook as a **pandas** dataframe as shown below:\n\n<img src=\"https://github.com/burtvialpando/CloudPakWorkshop/blob/master/CPD/images/InsertPandasDataFrame.png?raw=true\" width=\"300\" height=\"300\" align=\"middle\"/>\n\nThe interface comes up with a generic name, so it is good practice to rename the dataframe to match context of the use case. In this case, we will use df_churn."
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>AGE_GROUP</th>\n      <th>CHURNRISK</th>\n      <th>GENDER</th>\n      <th>STATUS</th>\n      <th>CHILDREN</th>\n      <th>ESTINCOME</th>\n      <th>HOMEOWNER</th>\n      <th>AGE</th>\n      <th>TAXID</th>\n      <th>...</th>\n      <th>LATITUDE</th>\n      <th>TOTALDOLLARVALUETRADED</th>\n      <th>TOTALUNITSTRADED</th>\n      <th>LARGESTSINGLETRANSACTION</th>\n      <th>SMALLESTSINGLETRANSACTION</th>\n      <th>PERCENTCHANGECALCULATION</th>\n      <th>DAYSSINCELASTLOGIN</th>\n      <th>DAYSSINCELASTTRADE</th>\n      <th>NETREALIZEDGAINS_YTD</th>\n      <th>NETREALIZEDLOSSES_YTD</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Young adult</td>\n      <td>Low</td>\n      <td>F</td>\n      <td>S</td>\n      <td>1</td>\n      <td>38000.00</td>\n      <td>N</td>\n      <td>24</td>\n      <td>147889187</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>59755.98</td>\n      <td>206</td>\n      <td>29877</td>\n      <td>2987</td>\n      <td>51.50</td>\n      <td>3</td>\n      <td>10</td>\n      <td>2987.799</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Adult</td>\n      <td>Low</td>\n      <td>M</td>\n      <td>M</td>\n      <td>2</td>\n      <td>29616.00</td>\n      <td>N</td>\n      <td>49</td>\n      <td>113772166</td>\n      <td>...</td>\n      <td>38.687261</td>\n      <td>29782.98</td>\n      <td>45</td>\n      <td>14891</td>\n      <td>1489</td>\n      <td>11.25</td>\n      <td>3</td>\n      <td>9</td>\n      <td>1489.149</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Adult</td>\n      <td>Low</td>\n      <td>M</td>\n      <td>M</td>\n      <td>0</td>\n      <td>19732.80</td>\n      <td>N</td>\n      <td>51</td>\n      <td>132420919</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>24812.48</td>\n      <td>22</td>\n      <td>12406</td>\n      <td>1240</td>\n      <td>5.50</td>\n      <td>1</td>\n      <td>9</td>\n      <td>1240.624</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Adult</td>\n      <td>High</td>\n      <td>M</td>\n      <td>S</td>\n      <td>2</td>\n      <td>96.33</td>\n      <td>N</td>\n      <td>56</td>\n      <td>700548452</td>\n      <td>...</td>\n      <td>32.531971</td>\n      <td>26132.61</td>\n      <td>32</td>\n      <td>13066</td>\n      <td>1306</td>\n      <td>8.00</td>\n      <td>3</td>\n      <td>5</td>\n      <td>0.000</td>\n      <td>1306.6305</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Young adult</td>\n      <td>High</td>\n      <td>F</td>\n      <td>M</td>\n      <td>2</td>\n      <td>52004.80</td>\n      <td>N</td>\n      <td>25</td>\n      <td>141013706</td>\n      <td>...</td>\n      <td>33.593192</td>\n      <td>5030.50</td>\n      <td>23</td>\n      <td>1257</td>\n      <td>125</td>\n      <td>3.45</td>\n      <td>2</td>\n      <td>19</td>\n      <td>0.000</td>\n      <td>251.5250</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows \u00d7 29 columns</p>\n</div>",
                        "text/plain": "   ID    AGE_GROUP CHURNRISK GENDER STATUS  CHILDREN  ESTINCOME HOMEOWNER  \\\n0   0  Young adult       Low      F      S         1   38000.00         N   \n1   1        Adult       Low      M      M         2   29616.00         N   \n2   2        Adult       Low      M      M         0   19732.80         N   \n3   3        Adult      High      M      S         2      96.33         N   \n4   4  Young adult      High      F      M         2   52004.80         N   \n\n   AGE      TAXID  ...   LATITUDE  TOTALDOLLARVALUETRADED TOTALUNITSTRADED  \\\n0   24  147889187  ...        NaN                59755.98              206   \n1   49  113772166  ...  38.687261                29782.98               45   \n2   51  132420919  ...        NaN                24812.48               22   \n3   56  700548452  ...  32.531971                26132.61               32   \n4   25  141013706  ...  33.593192                 5030.50               23   \n\n   LARGESTSINGLETRANSACTION SMALLESTSINGLETRANSACTION  \\\n0                     29877                      2987   \n1                     14891                      1489   \n2                     12406                      1240   \n3                     13066                      1306   \n4                      1257                       125   \n\n  PERCENTCHANGECALCULATION  DAYSSINCELASTLOGIN  DAYSSINCELASTTRADE  \\\n0                    51.50                   3                  10   \n1                    11.25                   3                   9   \n2                     5.50                   1                   9   \n3                     8.00                   3                   5   \n4                     3.45                   2                  19   \n\n   NETREALIZEDGAINS_YTD  NETREALIZEDLOSSES_YTD  \n0              2987.799                 0.0000  \n1              1489.149                 0.0000  \n2              1240.624                 0.0000  \n3                 0.000              1306.6305  \n4                 0.000               251.5250  \n\n[5 rows x 29 columns]"
                    },
                    "execution_count": 1,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Data Visualization is key step in data mining process that helps better understand data before it can be prepared for building ML models\n\nWe use Brunel library that comes preloaded within Watson Studio local environment to visualize the merged customer data. \n\nThe Brunel Visualization Language is a highly succinct and novel language that defines interactive data visualizations based on tabular data. The language is well suited for both data scientists and business users. More information about Brunel Visualization: https://github.com/Brunel-Visualization/Brunel/wiki\n\nTry Brunel visualization here: http://brunel.mybluemix.net/gallery_app/renderer"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df_churn_pd.dtypes"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df_churn_pd.describe()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "%brunel data('df_churn_pd') stack polar bar x(CHURNRISK) y(#count) color(CHURNRISK) bar tooltip(#all)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "%brunel data('df_churn_pd') bar x(STATUS) y(#count) color(STATUS) tooltip(#all) | stack bar x(STATUS) y(#count) color(CHURNRISK: pink-orange-yellow) bin(STATUS) sort(STATUS) percent(#count) label(#count) tooltip(#all) :: width=1200, height=350 "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "%brunel data('df_churn_pd') bar x(TOTALUNITSTRADED) y(#count) color(CHURNRISK: pink-gray-orange) sort(STATUS) percent(#count) label(#count) tooltip(#all) :: width=1200, height=350 "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "%brunel data('df_churn_pd') bar x(DAYSSINCELASTTRADE) y(#count) color(CHURNRISK: pink-gray-orange) sort(STATUS) percent(#count) label(#count) tooltip(#all) :: width=1200, height=350 "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id=\"prepare_data\"></a>\n## 3. Data preparation\n[Top](#top)\n\nData preparation is a very important step in machine learning model building. This is because the model can perform well only when the data it is trained on is good and well prepared. Hence, this step consumes bulk of data scientist's time spent building models.\n\nDuring this process, we identify categorical columns in the dataset. Categories needed to be indexed, which means the string labels are converted to label indices. These label indices and encoded using One-hot encoding to a binary vector with at most a single one-value indicating the presence of a specific feature value from among the set of all feature values. This encoding allows algorithms which expect continuous features to use categorical features.\n\nFinal step in the data preparation process is to assemble all the categorical and non-categorical columns into a feature vector. We use VectorAssembler for this. VectorAssembler is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### Use the DataFrameMapper class to declare transformations and variable imputations.\n\n* LabelBinarizer - Converts a categorical variable into a dummy variable (aka binary variable)\n* StandardScaler - Standardize features by removing the mean and scaling to unit variance, z = (x - u) / s\n\nSee docs: \n* https://github.com/scikit-learn-contrib/sklearn-pandas\n* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler\n* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html#sklearn.preprocessing.LabelBinarizer\n* https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Defining the categorical columns \ncategoricalColumns = ['GENDER', 'STATUS', 'HOMEOWNER', 'AGE_GROUP']\nnumericColumns = ['CHILDREN', 'ESTINCOME', 'TOTALDOLLARVALUETRADED', 'TOTALUNITSTRADED', 'LARGESTSINGLETRANSACTION', 'SMALLESTSINGLETRANSACTION', \n                          'PERCENTCHANGECALCULATION', 'DAYSSINCELASTLOGIN', 'DAYSSINCELASTTRADE', 'NETREALIZEDGAINS_YTD', 'NETREALIZEDLOSSES_YTD']"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "mapper = DataFrameMapper([\n    (['GENDER'], LabelBinarizer()),\n    (['STATUS'], LabelBinarizer()),\n    (['HOMEOWNER'], LabelBinarizer()),\n    (['AGE_GROUP'], LabelBinarizer()),\n    (['CHILDREN'],  StandardScaler()),\n    (['ESTINCOME'],  StandardScaler()),\n    (['TOTALDOLLARVALUETRADED'],  StandardScaler()),\n    (['TOTALUNITSTRADED'],  StandardScaler()),\n    (['LARGESTSINGLETRANSACTION'],  StandardScaler()),\n    (['SMALLESTSINGLETRANSACTION'],  StandardScaler()),\n    (['PERCENTCHANGECALCULATION'],  StandardScaler()),\n    (['DAYSSINCELASTLOGIN'],  StandardScaler()),\n    (['DAYSSINCELASTTRADE'],  StandardScaler()),\n    (['NETREALIZEDGAINS_YTD'],  StandardScaler()),\n    (['NETREALIZEDLOSSES_YTD'],  StandardScaler())], default=False)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df_churn_pd.columns"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Define input data to the model\nX = df_churn_pd.drop(['ID','CHURNRISK','AGE','TAXID','CREDITCARD','DOB','ADDRESS_1', 'ADDRESS_2', 'CITY', 'STATE', 'ZIP', 'ZIP4', 'LONGITUDE',\n       'LATITUDE'], axis=1)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "X.shape"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Define the target variable and encode with value between 0 and n_classes-1\nle = LabelEncoder()\ny = le.fit_transform(df_churn_pd['CHURNRISK'])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# split the data to training and testing set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=5)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id=\"build_model\"></a>\n## 4. Build Random Forest classification model\n[Top](#top)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We instantiate a decision-tree based classification algorithm, namely, RandomForestClassifier. Next we define a pipeline to chain together the various transformers and estimaters defined during the data preparation step before. Sklearn standardizes APIs for machine learning algorithms to make it easier to combine multiple algorithms into a single pipeline, or workflow.\n\nWe split original dataset into train and test datasets. We fit the pipeline to training data and apply the trained model to transform test data and generate churn risk class prediction"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "import warnings\nwarnings.filterwarnings(\"ignore\")"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Instantiate the Classifier\nrandom_forest = RandomForestClassifier(random_state=5)\n\n# Define the steps in the pipeline to sequentially apply a list of transforms and the estimator, i.e. RandomForestClassifier\nsteps = [('mapper', mapper),('RandonForestClassifier', random_forest)]\npipeline = sklearn.pipeline.Pipeline(steps)\n\n# train the model\nmodel=pipeline.fit( X_train, y_train )\n\nmodel"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "### call pipeline.predict() on your X_test data to make a set of test predictions\ny_prediction = model.predict( X_test )\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# show first 10 rows of predictions\ny_prediction[0:10,]"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# show first 10 rows of predictions with the corresponding labels\nle.inverse_transform(y_prediction)[0:10]"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Model results\n\nIn a supervised classification problem such as churn risk classification, we have a true output and a model-generated predicted output for each data point. For this reason, the results for each data point can be assigned to one of four categories:\n\n1. True Positive (TP) - label is positive and prediction is also positive\n2. True Negative (TN) - label is negative and prediction is also negative\n3. False Positive (FP) - label is negative but prediction is positive\n4. False Negative (FN) - label is positive but prediction is negative\n\nThese four numbers are the building blocks for most classifier evaluation metrics. A fundamental point when considering classifier evaluation is that pure accuracy (i.e. was the prediction correct or incorrect) is not generally a good metric. The reason for this is because a dataset may be highly unbalanced. For example, if a model is designed to predict fraud from a dataset where 95% of the data points are not fraud and 5% of the data points are fraud, then a naive classifier that predicts not fraud, regardless of input, will be 95% accurate. For this reason, metrics like precision and recall are typically used because they take into account the type of error. In most applications there is some desired balance between precision and recall, which can be captured by combining the two into a single metric, called the F-measure."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# display label mapping to assist with interpretation of the model results\nlabel_mapping=le.inverse_transform([0,1,2])\nprint('0: ', label_mapping[0])\nprint('1: ', label_mapping[1])\nprint('2: ', label_mapping[2])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "### test your predictions using sklearn.classification_report()\nreport = sklearn.metrics.classification_report( y_test, y_prediction )\n\n### and print the report\nprint(report)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "print('Accuracy:   ',sklearn.metrics.accuracy_score( y_test, y_prediction ))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### Get the column names of the transformed features"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "m_step=pipeline.named_steps['mapper']"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "m_step.transformed_names_"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "features = m_step.transformed_names_"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Get the features importance\nimportances = pipeline.named_steps['RandonForestClassifier'][1].feature_importances_\nindices = np.argsort(importances)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "plt.figure(1)\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='b',align='center')\nplt.yticks(range(len(indices)), (np.array(features))[indices])\nplt.xlabel('Relative Importance')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id=\"save_model\"></a>\n## 5. Save the model into WML Deployment Space\n[Top](#top)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Before we save the model we must create a deployment space. Watson Machine Learning provides deployment spaces where the user can save, configure and deploy their models. We can save models, functions and data assets in this space.\n\nThe steps involved for saving and deploying the model are as follows:\n\n1. Lookup the pre-created deployment space. \n2. Set this deployment space as the default space.\n3. Store the model pipeline in the deployment space. Enter the name for the model in the cell below. \n4. Deploy the saved model. Enter the deployment name in the cell below. \n5. Retrieve the scoring endpoint to score the model with a payload\n\nWe use the ibm_watson_machine_learning library to complete these steps. "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#!pip install ibm-watson-machine-learning==1.0.14\n#!pip uninstall -y ibm-watson-machine-learning\n#!pip install --no-cache-dir ibm-watson-machine-learning==1.0.14"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Specify a names for the space being created, the saved model and the model deployment\nspace_name = 'churnrisk_deployment_space'\n\nmodel_name = 'churnrisk_model_nb'\n\ndeployment_name = 'churnrisk_model_deployment'"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from ibm_watson_machine_learning import APIClient\n\n# create the WML credentials with the apikey\nwml_credentials = {\n                   \"url\": \"https://us-south.ml.cloud.ibm.com\",\n                   \"apikey\":\"INSERT YOUR APIKEY HERE\"\n                  }\n\nclient = APIClient(wml_credentials)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 5.1 Lookup Deployment Space"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "for space in client.spaces.get_details()['resources']:\n    if space_name in space['entity']['name']:\n        space_id = space['metadata']['id']\n        print(space_id)\n        client.set.default_space(space_id)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 5.2 Store the model in the deployment space"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# list all supported software specs\nclient.software_specifications.list()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# run this line if you do not know the version of scikit-learn that was used to build the model\n!pip show scikit-learn"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "software_spec_uid = client.software_specifications.get_uid_by_name('scikit-learn_0.22-py3.6')\n#software_spec_uid = client.software_specifications.get_uid_by_name('scikit-learn_0.20-py3.6')"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "metadata = {\n    client.repository.ModelMetaNames.NAME: model_name,\n    client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: software_spec_uid,\n    client.repository.ModelMetaNames.TYPE: \"scikit-learn_0.22\"\n}\n\nstored_model_details = client.repository.store_model(pipeline,\n                                               meta_props=metadata,\n                                               training_data=X_train,\n                                               training_target=y_train)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "stored_model_details"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 5.3 Create a deployment for the stored model"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# deploy the model\nmeta_props = {\n    client.deployments.ConfigurationMetaNames.NAME: deployment_name,\n    client.deployments.ConfigurationMetaNames.ONLINE: {}\n}\n\n# deploy the model\n\nmodel_uid = stored_model_details[\"metadata\"][\"id\"]\ndeployment_details = client.deployments.create( artifact_uid=model_uid, meta_props=meta_props)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 5.4 Score the model"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# retrieve the scoring endpoint\nscoring_endpoint = client.deployments.get_scoring_href(deployment_details)\n\nprint('Scoring Endpoint:   ',scoring_endpoint)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "scoring_deployment_id = client.deployments.get_uid(deployment_details)\nclient.deployments.get_details(scoring_deployment_id)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "payload = [{\"values\": [ ['Young adult','M','S', 2,56000, 'N', 5030, 23, 2257, 125, 3.45, 2, 19, 1200, 251]]}]"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "payload_metadata = {client.deployments.ScoringMetaNames.INPUT_DATA: payload}\n# score\npredictions = client.deployments.score(scoring_deployment_id, payload_metadata)\npredictions"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "# display label mapping to assist with interpretation of the model results\nlabel_mapping=le.inverse_transform([0,1,2])\nprint('0: ', label_mapping[0])\nprint('1: ', label_mapping[1])\nprint('2: ', label_mapping[2])"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Useful Helper Functions "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### Create download links for the test data .csv files for batch scoring and model evaluations"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Define functions to download as CSV or Excel\nfrom IPython.display import HTML\nimport pandas as pd\nimport base64, io\n\n# Download as CSV: data frame, optional title and filename\ndef create_download_link_csv(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    # generate in-memory CSV, then base64-encode it\n    csv = df.to_csv(index=False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Write the test data a .csv so that we can later use it for batch scoring\ncreate_download_link_csv(X_test,\"Download my data\",\"churn_risk_model_batch_score.csv\")"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Write the test data to a .csv so that we can later use it for evaluation\ncreate_download_link_csv(X_test,\"Download my data\",\"model_eval.csv\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### Save and restore the model using the joblib package"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Save the pipeline with joblib\n!pip install joblib\nimport joblib\nfilename = 'churnrisk_model.sav'\njoblib.dump(pipeline, filename)\n! ls -lrt"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Use joblib to restore the model and score it with the test data\nfilename = 'churnrisk_model.sav'\nloaded_model = joblib.load(filename)\nresult = loaded_model.score(X_test, y_test)\nprint(result)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### Save and restore the model using the pickle package"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Save the pipeline with pickle\nimport pickle\nfilename = 'churnrisk_model.pkl'\npickle.dump(model, open(filename, 'wb'))\n!ls -lrt"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Use pickle to restore the model and score it with the test data\nfilename = 'churnrisk_model.pkl'\nloaded_model = pickle.load(open(filename, 'rb'))\nresult = loaded_model.score(X_test, y_test)\nprint(result)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### Use the project_lib package to save the model to the project data assets where it can be downloaded"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from project_lib import Project\n# project id from project url \n# the id can be taken from the project url shown in the browser, \n# For example, the project id is 28f40464-f07e-43c4-94a0-f6100744bd3d in this notebook URL\n# https://dataplatform.cloud.ibm.com/analytics/notebooks/v2/3fed0ab0-2abe-4ff1-8aee-26481557e7c3?projectid=28f40464-f07e-43c4-94a0-f6100744bd3d&context=cpdaas\nproject_id = 'YOUR PROJECT ID'\n# Get the value of access token created earlier in the Project Settings \naccess_token = 'YOUR ACCESS TOKEN'\nproject = Project(None, project_id, access_token)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# prin project details of interest \npc = project.project_context\nprint('Project Name: {0}'.format(project.get_name()))\nprint('Project Description: {0}'.format(project.get_description()))\nprint('Project Bucket Name: {0}'.format(project.get_project_bucket_name()))\nprint('Project Assets (Connections): {0}'.format(project.get_assets(asset_type='connection')))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Save the models to object storage \nproject.save_data(data=pickle.dumps(pipeline),file_name='churn_risk.pkl',overwrite=True)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "**Last updated:** 10/11/2020 - Original Notebook by Anjali Shah, updated in later versions by Sidney Phoon. Final edits by Burt Vialpando and Kent Rubin - IBM.  Updated for the Virtual TechU Oct 2020 by Jim Van Oosten."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.7",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}